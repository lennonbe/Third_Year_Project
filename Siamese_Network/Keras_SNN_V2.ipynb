{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras_SNN_V2.ipynb","provenance":[{"file_id":"11erkpUcju3soKpprq8Y5Qdg0K5WejX4z","timestamp":1640523509564}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"18LZfMc1Z59mLUh-6W2R9GNXAzjedSI87","authorship_tag":"ABX9TyN+a1p1lob/2PMFHhTt7IuU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XzXWfJ3IN6Gg"},"outputs":[],"source":["#Import dependencies\n","import cv2\n","from PIL import Image\n","import os\n","import random\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","#Import tensorflow dependencies\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","source":["def make_embedding():\n","    input = Input(shape=(224,224,3), name='flood_image')\n","\n","    #First convulutional layer\n","    c1 = Conv2D(64, (10,10), activation='relu')(input)\n","    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n","\n","    #Second convulutional layer\n","    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n","    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n","\n","    #Third and final convulutional layer\n","    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n","    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n","\n","    #Final layer\n","    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n","    f1 = Flatten()(c4)\n","    d1 = Dense(4096, activation='sigmoid')(f1)\n","\n","    return Model(inputs=[input], outputs=[d1], name='embedding')"],"metadata":{"id":"MO8r006IOGnX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding = make_embedding()\n","embedding.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rsw0_s5-PP18","executionInfo":{"status":"ok","timestamp":1640792778233,"user_tz":0,"elapsed":2787,"user":{"displayName":"Daniel Lennon Beato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEJRsxk3v-ST_AHEFSN7dmbqrTIr81izMUeQKP_w=s64","userId":"10725049979396702399"}},"outputId":"8e147e8b-43a1-4a65-f272-6cb65bbccfe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"embedding\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flood_image (InputLayer)    [(None, 224, 224, 3)]     0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 215, 215, 64)      19264     \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 108, 108, 64)     0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 102, 102, 128)     401536    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 51, 51, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 48, 48, 128)       262272    \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 24, 24, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 21, 21, 256)       524544    \n","                                                                 \n"," flatten (Flatten)           (None, 112896)            0         \n","                                                                 \n"," dense (Dense)               (None, 4096)              462426112 \n","                                                                 \n","=================================================================\n","Total params: 463,633,728\n","Trainable params: 463,633,728\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Siamese L1 Distance class\n","class L1Dist(Layer):\n","    \n","    # Init method - inheritance\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","       \n","    # Similarity calculation\n","    def call(self, input_embedding, validation_embedding):\n","        return tf.math.abs(input_embedding - validation_embedding)"],"metadata":{"id":"JxKgXxGCR7Bv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_siamese_model(): \n","    \n","    # Anchor image input in the network\n","    input_image = Input(name='input_img', shape=(224,224,3))\n","    \n","    # Validation image in the network \n","    validation_image = Input(name='validation_img', shape=(224,224,3))\n","    \n","    # Combine siamese distance components\n","    siamese_layer = L1Dist()\n","    siamese_layer._name = 'distance'\n","    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n","    \n","    # Classification layer \n","    classifier = Dense(1, activation='sigmoid')(distances)\n","    \n","    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"],"metadata":{"id":"QcIIkladVfRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup paths\n","TRUE_PAIRINGS = '/content/drive/MyDrive/Third_Year_Project/Siamese_Network/Data_Pre_Processing_Siamese/Image_Pairs_Equal'\n","FALSE_PAIRINGS = '/content/drive/MyDrive/Third_Year_Project/Siamese_Network/Data_Pre_Processing_Siamese/Image_Pairs'\n","\n","import tensorflow.compat.v1 as tf1\n","\n","#Get our anchor, negative and positive data\n","#This is done by creating an iterator for this path up to file 500\n","pairs_true = tf.data.Dataset.list_files(TRUE_PAIRINGS +'/*').take(150)\n","pairs_false = tf.data.Dataset.list_files(FALSE_PAIRINGS +'/*').take(150)\n","\n","def preprocess(file_path):\n","    \n","    # Read in image from file path\n","    byte_img = tf.io.read_file(file_path)\n","    # Load in the image \n","    img = tf.io.decode_jpeg(byte_img)\n","    \n","    # Preprocessing steps - resizing the image to be 224x224x3\n","    img = tf.image.resize(img, (224,224))\n","    # Scale image to be between 0 and 1 \n","    img = img / 255.0\n","\n","    # Return image\n","    return img\n","\n","positives = tf.data.Dataset.zip((pairs_true, tf.data.Dataset.from_tensor_slices(tf.ones(len(pairs_true)))))\n","negatives = tf.data.Dataset.zip((pairs_false, tf.data.Dataset.from_tensor_slices(tf.zeros(len(pairs_false)))))\n","data = positives.concatenate(negatives)\n","\n","samples = data.as_numpy_iterator()"],"metadata":{"id":"YdUNBuvt_ZZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_twin(directory, label):\n","    \n","    count = 1\n","    for filename in os.listdir(directory):\n","\n","        if count == 1:\n","          input_img = directory.decode() + '/' + filename.decode()\n","          count += 1\n","        else:\n","          validation_img = directory.decode() + '/' + filename.decode()\n","          break\n","    \n","    if \"Image_Pairs_Equal\" in directory.decode() != -1:\n","        arr = np.ones((224,224, 3), dtype=int)\n","    else:\n","        arr = np.zeros((224,224, 3), dtype=int)\n","    \n","    return(preprocess(input_img), preprocess(validation_img), arr)\n","\n","preprocessed_data = []\n","for element in data.as_numpy_iterator():\n","    \n","    '''\n","    print(element[0])\n","    for filename in os.listdir(element[0].decode()):\n","        print(element[0].decode() + '/' + filename)\n","    '''\n","\n","    preprocessed_data.append(preprocess_twin(element[0], element[1]))\n","\n","b = tf.convert_to_tensor(preprocessed_data)"],"metadata":{"id":"seEJEKFwOAsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocessed_data\n","\n","data_final = tf.data.Dataset.from_tensors(b)"],"metadata":{"id":"CqN0jf3eNGz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build dataloader pipeline\n","print(tf.executing_eagerly())\n","#data = data.map(preprocess_twin)\n","data_final = data_final.cache()\n","data_final = data_final.cache()\n","data_final = data_final.shuffle(buffer_size=10000)\n","\n","# Training partition\n","train_data = data_final.take(round(len(data)*.7))\n","train_data = train_data.batch(16)\n","train_data = train_data.prefetch(8)\n","\n","# Testing partition\n","test_data = data_final.skip(round(len(data)*.7))\n","test_data = test_data.take(round(len(data)*.3))\n","test_data = test_data.batch(16)\n","test_data = test_data.prefetch(8)"],"metadata":{"id":"solzK22ZPMzp","executionInfo":{"status":"ok","timestamp":1640793064104,"user_tz":0,"elapsed":4,"user":{"displayName":"Daniel Lennon Beato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEJRsxk3v-ST_AHEFSN7dmbqrTIr81izMUeQKP_w=s64","userId":"10725049979396702399"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"da362d57-2198-4f75-eef7-3b47c0524223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["siamese_model = make_siamese_model()\n","siamese_model.summary()\n","binary_cross_loss = tf.losses.BinaryCrossentropy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMtkLIprebVt","executionInfo":{"status":"ok","timestamp":1640793064104,"user_tz":0,"elapsed":3,"user":{"displayName":"Daniel Lennon Beato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEJRsxk3v-ST_AHEFSN7dmbqrTIr81izMUeQKP_w=s64","userId":"10725049979396702399"}},"outputId":"936c0387-6ad9-4eed-eed3-6b06cde0a260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"SiameseNetwork\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_img (InputLayer)         [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," validation_img (InputLayer)    [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," embedding (Functional)         (None, 4096)         463633728   ['input_img[0][0]',              \n","                                                                  'validation_img[0][0]']         \n","                                                                                                  \n"," distance (L1Dist)              (None, 4096)         0           ['embedding[0][0]',              \n","                                                                  'embedding[1][0]']              \n","                                                                                                  \n"," dense_1 (Dense)                (None, 1)            4097        ['distance[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 463,637,825\n","Trainable params: 463,637,825\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"],"metadata":{"id":"gR184RMJebl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"],"metadata":{"id":"VMKwDjoHeeVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_batch = train_data.as_numpy_iterator()"],"metadata":{"id":"Ijc2KylTerdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_1 = test_batch.next()"],"metadata":{"id":"dcyJU2EcetoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#X = batch_1[:2]\n","\n","#(1, 300, 3, 224, 224, 3) is our data pipeline batch shape after pre processing\n","print(batch_1[0][0][0])\n","print(batch_1[0][0][1])\n","print(batch_1[0][0][2])\n","\n","#This means we would want to iterate through our data as follows batch_1[0][iter][n] where iter is the dimension we iterate"],"metadata":{"id":"hEtJmNTWetvH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640793180288,"user_tz":0,"elapsed":354,"user":{"displayName":"Daniel Lennon Beato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEJRsxk3v-ST_AHEFSN7dmbqrTIr81izMUeQKP_w=s64","userId":"10725049979396702399"}},"outputId":"e0ddfe19-1a41-4fea-97e8-f3c4efc67e85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0.46866748 0.4916767  0.43969586]\n","  [0.35944375 0.4055022  0.3505002 ]\n","  [0.36060423 0.45514205 0.32765105]\n","  ...\n","  [0.00754305 0.15016012 0.03931506]\n","  [0.00896353 0.16334508 0.0242096 ]\n","  [0.00482188 0.15028018 0.03833527]]\n","\n"," [[0.36012402 0.43561423 0.32743093]\n","  [0.35516208 0.41770712 0.35696283]\n","  [0.3690276  0.5188876  0.3679672 ]\n","  ...\n","  [0.00576215 0.14513765 0.05054046]\n","  [0.00392157 0.15258092 0.03503397]\n","  [0.00392157 0.11730656 0.04461879]]\n","\n"," [[0.3487395  0.41194475 0.31522608]\n","  [0.34281713 0.4086835  0.30560225]\n","  [0.3639856  0.4707083  0.34371752]\n","  ...\n","  [0.05520188 0.18359266 0.09359695]\n","  [0.00386153 0.12184884 0.04915943]\n","  [0.00338138 0.1081227  0.02156876]]\n","\n"," ...\n","\n"," [[0.5331173  0.5295966  0.5475432 ]\n","  [0.6413787  0.62907445 0.6858768 ]\n","  [0.6101643  0.586595   0.61606616]\n","  ...\n","  [0.21698573 0.10938196 0.09395559]\n","  [0.15936351 0.07018778 0.05558259]\n","  [0.20730372 0.14359798 0.12859315]]\n","\n"," [[0.6222102  0.55578464 0.59774095]\n","  [0.74547726 0.75488096 0.7811109 ]\n","  [0.6356538  0.6200476  0.6412563 ]\n","  ...\n","  [0.20756346 0.12471021 0.18161272]\n","  [0.17819232 0.1412166  0.11960825]\n","  [0.21546681 0.12695172 0.16056484]]\n","\n"," [[0.67719096 0.67478925 0.68149227]\n","  [0.74968356 0.7511248  0.76323026]\n","  [0.6920812  0.70666736 0.6961429 ]\n","  ...\n","  [0.20944418 0.11764921 0.09401782]\n","  [0.25194144 0.17991385 0.16002622]\n","  [0.26544315 0.21560492 0.22408608]]]\n","[[[0.997479   0.997479   0.99607843]\n","  [1.         0.99981993 0.997479  ]\n","  [1.         1.         0.997479  ]\n","  ...\n","  [0.02603044 0.14901961 0.03251289]\n","  [0.02372953 0.12502992 0.02623047]\n","  [0.02492997 0.11782712 0.0267704 ]]\n","\n"," [[0.99607843 0.99607843 0.99607843]\n","  [0.99607843 0.99607843 0.99607843]\n","  [0.99607843 0.99607843 0.99607843]\n","  ...\n","  [0.03057191 0.12941141 0.03371325]\n","  [0.0271508  0.11424553 0.02771113]\n","  [0.01988795 0.10434126 0.03109244]]\n","\n"," [[0.99607843 0.99607843 0.99607843]\n","  [0.9997199  0.99893963 0.99607843]\n","  [0.99607843 0.99607843 0.99607843]\n","  ...\n","  [0.05099986 0.1369146  0.0553821 ]\n","  [0.02060827 0.09971989 0.03143258]\n","  [0.02352941 0.11574516 0.0303118 ]]\n","\n"," ...\n","\n"," [[0.39488098 0.3892981  0.38511717]\n","  [0.54352075 0.54278094 0.5811977 ]\n","  [0.51964813 0.46702567 0.50792295]\n","  ...\n","  [0.0707259  0.03931508 0.03521202]\n","  [0.12308974 0.06360581 0.06052425]\n","  [0.18357469 0.1658281  0.16468616]]\n","\n"," [[0.48607612 0.44351935 0.45010167]\n","  [0.6819927  0.68421346 0.6859342 ]\n","  [0.59389645 0.58529294 0.58097124]\n","  ...\n","  [0.03921593 0.02941188 0.03193288]\n","  [0.12317026 0.06704763 0.08313344]\n","  [0.18841527 0.15624325 0.12166987]]\n","\n"," [[0.60544044 0.59307516 0.5929349 ]\n","  [0.6120477  0.6056048  0.59424114]\n","  [0.61156666 0.59702164 0.586877  ]\n","  ...\n","  [0.03977625 0.02857187 0.02721109]\n","  [0.16114594 0.10640463 0.10278301]\n","  [0.24659415 0.19845651 0.18823215]]]\n","[[[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  ...\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  ...\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  ...\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," ...\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  ...\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  ...\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  ...\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]]\n"]}]},{"cell_type":"code","source":["tf.losses.BinaryCrossentropy??"],"metadata":{"id":"E3AN-BU2e1LD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(pair):\n","    \n","    # Record all of our operations \n","    with tf.GradientTape() as tape:     \n","        # (1, 300, 3, 224, 224, 3) is our data pipeline batch shape after pre processing\n","        # Get anchor and positive/negative image\n","        X1 = pair[0]\n","        X2 = pair[1]\n","        # Get label\n","        y = pair[2]\n","        \n","        # Forward pass\n","        yhat = siamese_model(X1, X2, training=True)\n","        # Calculate loss\n","        loss = binary_cross_loss(y, yhat)\n","    print(loss)\n","        \n","    # Calculate gradients\n","    grad = tape.gradient(loss, siamese_model.trainable_variables)\n","    \n","    # Calculate updated weights and apply to siamese model\n","    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n","        \n","    # Return loss\n","    return loss"],"metadata":{"id":"Rs1igi4Ge3yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import metric calculations\n","from tensorflow.keras.metrics import Precision, Recall"],"metadata":{"id":"0SBvWUYZe7VH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(data, EPOCHS):\n","    # Loop through epochs\n","    for epoch in range(1, EPOCHS+1):\n","        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n","        progbar = tf.keras.utils.Progbar(len(data))\n","        \n","        # Creating a metric object \n","        r = Recall()\n","        p = Precision()\n","        \n","        # Loop through each batch\n","        for idx, batch in enumerate(data):\n","            for e in batch:\n","                for i in e:                    \n","                    # Run train step here\n","                    loss = train_step(i)\n","                    print(i[0])\n","                    print(i[1])\n","                    yhat = siamese_model.predict(i[0],i[1])\n","                    r.update_state(i[2], yhat)\n","                    p.update_state(i[2], yhat) \n","                    progbar.update(idx+1)\n","        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n","        \n","        # Save checkpoints\n","        if epoch % 10 == 0: \n","            checkpoint.save(file_prefix=checkpoint_prefix)"],"metadata":{"id":"SJ-u1OPue9td"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"M5F45G57zxLs","executionInfo":{"status":"ok","timestamp":1640793287814,"user_tz":0,"elapsed":8624,"user":{"displayName":"Daniel Lennon Beato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEJRsxk3v-ST_AHEFSN7dmbqrTIr81izMUeQKP_w=s64","userId":"10725049979396702399"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ed63176-c641-4d02-a5b9-f6926cad4a19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["EPOCHS = 50"],"metadata":{"id":"iBGv2-btfADb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(train_data, EPOCHS)"],"metadata":{"id":"T1LjjyvmfCK3","executionInfo":{"status":"error","timestamp":1640794909912,"user_tz":0,"elapsed":901,"user":{"displayName":"Daniel Lennon Beato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEJRsxk3v-ST_AHEFSN7dmbqrTIr81izMUeQKP_w=s64","userId":"10725049979396702399"}},"colab":{"base_uri":"https://localhost:8080/","height":504},"outputId":"a6ee6456-5a44-411c-f663-d86b37491853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1/50\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-3a6c1daed93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-49-bcfa53f521e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, EPOCHS)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0;31m# Run train step here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-47-2c7b5d5ed0be>\", line 14, in train_step  *\n        yhat = siamese_model(X1, X2, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 199, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"SiameseNetwork\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'strided_slice:0' shape=(224, 224, 3) dtype=float32>]\n"]}]},{"cell_type":"code","source":["# Get a batch of test data\n","test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n","\n","y_hat = siamese_model.predict([test_input, test_val])\n","\n","print(y_hat)\n","\n","# Post processing the results \n","[1 if prediction > 0.5 else 0 for prediction in y_hat ]\n","\n","# Creating a metric object \n","m = Recall()\n","\n","# Calculating the recall value \n","m.update_state(y_true, y_hat)\n","\n","# Return Recall Result\n","m.result().numpy()\n","\n","# Creating a metric object \n","m = Precision()\n","\n","# Calculating the recall value \n","m.update_state(y_true, y_hat)\n","\n","# Return Recall Result\n","m.result().numpy()\n","\n","r = Recall()\n","p = Precision()\n","\n","for test_input, test_val, y_true in test_data.as_numpy_iterator():\n","    yhat = siamese_model.predict([test_input, test_val])\n","    r.update_state(y_true, yhat)\n","    p.update_state(y_true,yhat) \n","\n","print(r.result().numpy(), p.result().numpy())\n","\n","# Set plot size \n","plt.figure(figsize=(10,8))\n","\n","# Set first subplot\n","plt.subplot(1,2,1)\n","plt.imshow(test_input[0])\n","\n","# Set second subplot\n","plt.subplot(1,2,2)\n","plt.imshow(test_val[0])\n","\n","# Renders cleanly\n","plt.show()"],"metadata":{"id":"Nbc6qPrCfD74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save weights\n","siamese_model.save('/content/drive/MyDrive/Third_Year_Project/Siamese_Network/siamesemodelv2.h5')\n","L1Dist\n","# Reload model \n","siamese_model = tf.keras.models.load_model('/content/drive/MyDrive/Third_Year_Project/Siamese_Network/siamesemodelv2.h5', \n","                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})\n","\n","# Make predictions with reloaded model\n","siamese_model.predict([test_input, test_val])"],"metadata":{"id":"PdijmlJtfe3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View model summary\n","siamese_model.summary()"],"metadata":{"id":"SAyMjrHZffyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import h5py\n","\n","filename = \"/content/drive/MyDrive/Third_Year_Project/Siamese_Network/siamesemodelv2.h5\"\n","\n","h5 = h5py.File(filename,'r')\n","\n","futures_data = h5  # VSTOXX futures data\n","\n","print(h5)\n","\n","h5.close()"],"metadata":{"id":"WlUcKQ3MxLPC"},"execution_count":null,"outputs":[]}]}